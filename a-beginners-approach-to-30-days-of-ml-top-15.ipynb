{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/evi125/a-beginners-approach-to-30-days-of-ml-top-15?scriptVersionId=88610181\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# 30 Days of ML\n\nHi, I'm Evelin. I'm a beginner without any programming or machine learning background and this is the first data science competition I'm participating in (after the House Prices competition we practised on while doing the Machine Learning courses).\n\nI made quite a few notebooks with many many versions of them. Being a beginner and not knowing good techniques I tested most things by making small changes and comparing the result of the cross validation score and the public score. If both the CV score and the public score got better, I kept the change, if only one of them, I didn't.\n\nAfter checking for missing values, I tried a few things for numerical and categorical columns: StandardScaler(), MinMaxScaler(); Normalizer(), OrdinalEncoder(), OneHotEncoder(); KBinsDiscretizer(); KMeans(); PCA; TargetEncoder() and also removing columns and using different scaling or encoding for different features. StandardScaler() for numerical columns and OneHotEncoder() gave the best results.\n\nI followed the same approach for selecting the hyperparameters for XGBoost as well. First, I ran a GridSearch with a few parameters. Then I tried optimizing the parameters around the best score one-by-one or in groups of 2 or 3. Eg. if I got n_estimator=1000, learning_rate=0.08, max_depth=4 as best parameters from the GridSearch, then next round I tried n_estimator= 900, 1000, 1100. Then, if I got 1100, I tried 1050, 1100, 1200 next... etc etc. Then, I added more parameters as I learnt about them (eg alpha, lambda). I went through all the parameters I used in my models this way. I know it's very inefficient, but it helped me reach a public score of 0.71825 and I was about 400th on the public leaderboard that time, so I was very happy with my results.\n\nAfter Abhishek's optimization and stacking videos came out, I learnt about Optuna and StackingRegressor(). Optuna often gave worse results than my tedious optimization technique, but stacking helped a lot improving my score. Unfortunately, it took so much time to run all the models and the cv as well, that I stopped using cv and only relied on my public score. That was a mistake and I believe that's the reason I fell more than 200 places on the private leaderboard.\n\n(I've also experimented with using a Pipeline and I loved it, but I got worse score using it, probably because I couldn't figure out how to use early_stopping for XGBoost with a pipeline, so I ditched it)\n\n\n**Resources I've used for this notebook:**\n\nPython, Intro to ML, Intermediate ML, Data Visualization mini courses\n\nDocumentations: Sklearn, XGBoost, LGBM, etc.\n\nThe [Getting Started with 30 Days of ML Competition](https://www.kaggle.com/alexisbcook/getting-started-with-30-days-of-ml-competition) notebook by [Alexis Cook](https://www.kaggle.com/alexisbcook)\n\nI used [this notebook](https://www.kaggle.com/garylucn/top-9-house-price/notebook) by [Gary Lu]( https://www.kaggle.com/garylucn) for Optuna and Stacking (I was struggling with using these techniques until I found this notebook, so thank you so much for sharing it!))\n\n[Abhishek Takur](https://www.kaggle.com/abhishek)'s  [youtube videos](https://www.youtube.com/watch?v=_55G24aghPY&list=PL98nY_tJQXZnP-k3qCDd1hljVSciDV9_N)\n\n[Scikit-optimize for LightGBM Tutorial with Luca Massaron](https://www.youtube.com/watch?v=AFtjWuwqpSQ&list=PLqFaTIg4myu9uAPsqXBBZRr8kcj9IvAIf&index=3) video by [Luca Massaron](https://www.kaggle.com/lucamassaron)\n\nI also used my own Housing Prices notebooks for which I learnt techniques and got inspiration from the following notebooks:\n\n* [House Prices: Pipeline & Cross-Validation](https://www.kaggle.com/sergejnuss/house-prices-pipeline-cross-validation) by [Sergej Nuss](https://www.kaggle.com/sergejnuss) for Pipelines\n* [Housing Prices Competition: Clear and Concise Exploratory Data Analysis](https://www.kaggle.com/korfanakis/housing-prices-clear-and-concise-eda) by [Orfanakis Konstantinos](https://www.kaggle.com/korfanakis) for Data Visualization, Data Analysis\n* [Housing Prices: A Simple Approach to Top 2%](https://www.kaggle.com/korfanakis/housing-prices-a-simple-approach-to-top-2) by [Orfanakis Konstantinos](https://www.kaggle.com/korfanakis)\n* [Data Science Workflow TOP 2% (with Tuning)](https://www.kaggle.com/angqx95/data-science-workflow-top-2-with-tuning) by [aqx](https://www.kaggle.com/angqx95)\n* [\nTop 1% Approach: EDA, New Models and Stacking](https://www.kaggle.com/datafan07/top-1-approach-eda-new-models-and-stacking) by [Ertuğrul Demir](https://www.kaggle.com/datafan07)\n\n\n\nThe [Statquest](https://www.youtube.com/user/joshstarmer) youtube channel by Josh Starmer helped me understand a lot of statistical and machine learning concepts.\n\n\nAlso [Attila Ambrus](https://www.kaggle.com/ambrusattila) was very kind and spent a lot of time answering my never ending questions in the Hungarian 30 Days of ML Discord community. Köszönöm a sok segítséget!!!!\n\n\nThank you everyone for sharing lots of valuable information and making my learning process a lot of fun!\n\n","metadata":{}},{"cell_type":"markdown","source":"**(Note: this notebook only includes my final work. It doesn't include everything I tried (eg. checking for missing values, scaling, encoding, removing columns, etc.), and tuning with GridSearch and Optuna.)**","metadata":{}},{"cell_type":"markdown","source":"# Importing necessary libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\n\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import StackingRegressor\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:25:04.703694Z","iopub.execute_input":"2021-09-02T13:25:04.704245Z","iopub.status.idle":"2021-09-02T13:25:07.152409Z","shell.execute_reply.started":"2021-09-02T13:25:04.704139Z","shell.execute_reply":"2021-09-02T13:25:07.151187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading and exploring the data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/30-days-of-ml/train.csv\", index_col=0)\ntest = pd.read_csv(\"../input/30-days-of-ml/test.csv\", index_col=0)\n\nprint(train.shape)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:25:07.153888Z","iopub.execute_input":"2021-09-02T13:25:07.154187Z","iopub.status.idle":"2021-09-02T13:25:11.337007Z","shell.execute_reply.started":"2021-09-02T13:25:07.154157Z","shell.execute_reply":"2021-09-02T13:25:11.336009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Numerical data","metadata":{}},{"cell_type":"code","source":"cont_data = train.select_dtypes(exclude='object')\n\nfig = plt.figure(figsize = (20, 15))\nfor i, col in enumerate(cont_data):\n    plt.subplot(3, 5, i + 1)\n    sns.histplot(x= col, data= train)\n    plt.xticks(rotation = 90)\nfig.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Categorical data","metadata":{}},{"cell_type":"code","source":"cat_data = train.select_dtypes(include='object')\n\nfig = plt.figure(figsize = (20, 10))\nfor i, col in enumerate(cat_data):\n    plt.subplot(2, 5, i + 1)\n    sns.countplot(x= col, data= train)\n    plt.xticks(rotation = 90)\nfig.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation between numerical features and the target","metadata":{}},{"cell_type":"code","source":"correlations = train.select_dtypes(exclude=['object']).corr()\ncorrelations = correlations[['target']].sort_values(by=['target'], ascending=False)\ncorrelations","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(20,30))\nfor index, column in enumerate(cont_data.columns):\n    plt.subplot(8,5, index+1)\n    sns.regplot(x=column, y='target', data= train)\nfig.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Multicorrelation","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize= (20,20))\nmulticorr = train.corr()\nsns.heatmap(multicorr, cmap=\"crest\", annot=True, fmt= \".3f\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There isn't strong correlation neither between the features nor any of the features and the target. I wonder if that's why none of the scaling/encoding/feature engineering techniques worked for me.","metadata":{}},{"cell_type":"markdown","source":"# Preparing the data","metadata":{}},{"cell_type":"code","source":"# Separate target from features\ny = train['target']\nfeatures = train.drop(['target'], axis=1)\n\nobject_cols = [col for col in features.columns if 'cat' in col]\nnumber_cols = [col for col in features.columns if 'cont' in col]\n\nX = features.copy()\nX_test = test.copy()\n\n# Encoding\n\nOH_encoder = OneHotEncoder(sparse=False)\n\nOH_cols = pd.DataFrame(OH_encoder.fit_transform(X[object_cols]))\nOH_test_cols = pd.DataFrame(OH_encoder.transform(X_test[object_cols]))\n\n# OH_cols.index = X[object_cols].index\n# OH_test_cols.index = X_test[object_cols].index\n\nnum = X.drop(object_cols, axis=1)\nnum_t = X_test.drop(object_cols, axis=1)\n\n# Scaling\n\nscaler = StandardScaler()\n\nscaled_cols = scaler.fit_transform(X[number_cols])\nscaled_test_cols = scaler.transform(X_test[number_cols])\n\nscaled_cols = pd.DataFrame(scaled_cols, columns=number_cols)\nscaled_test_cols = pd.DataFrame(scaled_test_cols, columns=number_cols)\n\n# Concat\n\npreprocessed_X = pd.concat([OH_cols, scaled_cols], axis=1)\npreprocessed_test_X = pd.concat([OH_test_cols, scaled_test_cols], axis=1)\n\n# Preview the ordinal-encoded features\npreprocessed_X.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:25:31.780081Z","iopub.execute_input":"2021-09-02T13:25:31.780461Z","iopub.status.idle":"2021-09-02T13:25:34.644108Z","shell.execute_reply.started":"2021-09-02T13:25:31.780426Z","shell.execute_reply":"2021-09-02T13:25:34.643387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models\n\nI got the first 3 XGBoost models using GridSearch and tuning the parameters one-by-one or in groups of two.\n\nFor the 4th XGBoost and LGBM models I used Optuna, but the LGBM model made the stack perform worse, so I'm leaving it out.\n\nThe RandomForest and GradientBoostingRegressor models are from Abhishek's video.","metadata":{}},{"cell_type":"markdown","source":"(Note: I know preprocessed_X should be just X, but I don't want to risk missing one out and creating errors, so I'm leaving it this way for now. I'll be more careful naming variables next time.)","metadata":{}},{"cell_type":"code","source":"RANDOM_SEED = 1215","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:25:39.117868Z","iopub.execute_input":"2021-09-02T13:25:39.118334Z","iopub.status.idle":"2021-09-02T13:25:39.122618Z","shell.execute_reply.started":"2021-09-02T13:25:39.118298Z","shell.execute_reply":"2021-09-02T13:25:39.121983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb1_params = {#'tree_method': 'gpu_hist',\n              'n_estimators': 1100,\n              'learning_rate': 0.08,\n              'max_depth': 4,\n              'min_child_weight': 1, \n              'subsample': 0.8, \n              'colsample_bytree': 0.5, \n              'reg_alpha': 5,\n              'reg_lambda': 1,\n              'gamma': 0,\n             }\n\nxgbr1 = xgb.XGBRegressor(random_state=RANDOM_SEED, **xgb1_params)\nxgbr1.fit(preprocessed_X, y, early_stopping_rounds =5, eval_set=[(preprocessed_X, y)], verbose=500)","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:26:45.667327Z","iopub.execute_input":"2021-09-02T13:26:45.667958Z","iopub.status.idle":"2021-09-02T13:29:40.402394Z","shell.execute_reply.started":"2021-09-02T13:26:45.667923Z","shell.execute_reply":"2021-09-02T13:29:40.400374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb2_params = {#'tree_method': 'gpu_hist',\n              'n_estimators': 1500,\n              'learning_rate': 0.08,\n              'max_depth': 4,\n              'min_child_weight': 1, \n              'subsample': 0.9, \n              'colsample_bytree': 0.2, \n              'reg_alpha': 8,\n              'reg_lambda': 20,\n              'gamma': 0,\n             }\n\nxgbr2 = xgb.XGBRegressor(random_state=RANDOM_SEED, **xgb2_params)\nxgbr2.fit(preprocessed_X, y, early_stopping_rounds =5, eval_set=[(preprocessed_X, y)], verbose=500)","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:29:40.40337Z","iopub.status.idle":"2021-09-02T13:29:40.403755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb3_params = {#'tree_method': 'gpu_hist',\n               'n_estimators': 1700,\n              'learning_rate': 0.08,\n              'max_depth': 4,\n              'min_child_weight': 1, \n              'subsample': 0.9, \n              'colsample_bytree': 0.1, \n              'reg_alpha': 10,\n              'reg_lambda': 20,\n              'gamma': 0\n             }\n\nxgbr3 = xgb.XGBRegressor(random_state=RANDOM_SEED, **xgb3_params)\nxgbr3.fit(preprocessed_X, y, early_stopping_rounds =5, eval_set=[(preprocessed_X, y)], verbose=500)","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:29:40.40733Z","iopub.status.idle":"2021-09-02T13:29:40.407985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb4_params = {#'tree_method': 'gpu_hist',\n              'n_estimators': 4366, \n               'max_depth': 3, \n               'learning_rate': 0.05435500605120945, \n               'gamma': 0.492373667901573, \n               'min_child_weight': 5.02962746238382, \n               'subsample': 0.48512472393136913, \n               'colsample_bytree': 0.16115615922020954, \n               'reg_alpha': 6.564178028196104, \n               'reg_lambda': 8.21636933472606\n             }\n\nxgbr4 = xgb.XGBRegressor(random_state=RANDOM_SEED, **xgb4_params)\nxgbr4.fit(preprocessed_X, y, early_stopping_rounds =5, eval_set=[(preprocessed_X, y)], verbose=500)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lgb_params = {'num_leaves': 38, 'n_estimators': 897, 'max_depth': 5, \n#               'learning_rate': 0.08612849100285823, 'min_child_weight': 1.5455034368368281, \n#               'subsample': 0.3757097543793422, 'colsample_bytree': 0.10598238005270319, \n#               'reg_alpha': 7.677631589902765, 'reg_lambda': 4.300612342722245}\n\n# lgbr = lgb.LGBMRegressor(random_state=RANDOM_SEED, **lgb_params)\n# lgbr.fit(preprocessed_X, y, early_stopping_rounds =5, eval_set=[(preprocessed_X, y)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"randomforest_params = {'n_estimators': 500,  'max_depth': 3}\nrf = RandomForestRegressor(n_jobs=-1, random_state=RANDOM_SEED, **randomforest_params)\nrf.fit(preprocessed_X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbr_params = {'n_estimators': 500,  'max_depth': 3}\ngbr = GradientBoostingRegressor(random_state=RANDOM_SEED, **gbr_params)\ngbr.fit(preprocessed_X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stacking","metadata":{}},{"cell_type":"code","source":"stack = StackingRegressor(\n    estimators=[\n        ('xgbr1', xgbr1),\n        ('xgbr2', xgbr2),\n        ('xgbr3', xgbr3),\n        ('xgbr4', xgbr4),\n        #('lgbr', lgbr),\n        ('rf', rf),\n        ('gbr', gbr)\n    ],\n    cv=5)\nstack.fit(preprocessed_X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As I mentioned earlier I made the mistake of not using cross validation for the stacks, because it took a lot of time. I used cross_val_score with cv=5 for GridSearch and Optuna.**","metadata":{}},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"print('Predict submission')\nsubmission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\nsubmission.iloc[:,1] = stack.predict(preprocessed_test_X)\n\nsubmission.to_csv('my_submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}